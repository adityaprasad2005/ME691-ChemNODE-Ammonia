{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityaprasad2005/ME691-ChemNODE-Ammonia/blob/main/notebooks/phase3-model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP-nDLodolLc",
        "outputId": "048b8e8e-80dc-44f7-dd10-506ec6086f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq) (2.9.0+cu126)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.3)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "# 1. Install torchdiffeq\n",
        "# (Run this in your terminal)\n",
        "!pip install torchdiffeq\n",
        "\n",
        "from torchdiffeq import odeint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sK71dT7qjgK",
        "outputId": "2b865e7b-f171-498f-c5c9-29e36b5e7a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the Configuration\n",
        "\n",
        "# File paths\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/SciML_Project\"\n",
        "\n",
        "NPZ_FILE = os.path.join(DRIVE_PATH, 'training_data.npz')\n",
        "PARAMS_FILE = os.path.join(DRIVE_PATH, 'normalization_params.json')\n",
        "MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, 'chem_node_net_3lay_128dim.pth')\n",
        "\n",
        "# Training hyperparameters\n",
        "N_EPOCHS = 500\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-3\n",
        "HIDDEN_DIMS = 128\n",
        "\n",
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Load data to get model dimensions\n",
        "\n",
        "try:\n",
        "    with np.load(NPZ_FILE) as data:\n",
        "        all_trajectories = data['trajectories']\n",
        "        t_points = data['times']\n",
        "\n",
        "    # Get the number of features (e.g., 32 for your ammonia mech)\n",
        "    # Shape is (num_sims, num_timesteps, num_features)\n",
        "    N_FEATURES = all_trajectories.shape[2]\n",
        "\n",
        "    print(f\"Data loaded. Found {N_FEATURES} features (species + temp).\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find '{NPZ_FILE}'.\")\n",
        "    print(\"Please make sure you have run Phase 2.\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLNeTitKotnC",
        "outputId": "198ae1ae-0c44-4472-e770-ab3c1c49bac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Data loaded. Found 32 features (species + temp).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Step 1: The Derivative Network\n",
        "\n",
        "class ChemNet(nn.Module):\n",
        "    def __init__(self, input_features, hidden_features):\n",
        "        super(ChemNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_features, hidden_features),\n",
        "            nn.ELU(),  # Exponential Linear Unit\n",
        "            nn.Linear(hidden_features, hidden_features),\n",
        "            nn.ELU(),\n",
        "            # nn.Linear(hidden_features, hidden_features),\n",
        "            # nn.ELU(),\n",
        "            nn.Linear(hidden_features, input_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, t, y):\n",
        "        # The 'odeint' solver requires the network's forward\n",
        "        # method to accept (time, state) as arguments.\n",
        "        # We don't use 't' here, but it's a required argument.\n",
        "        return self.net(y)"
      ],
      "metadata": {
        "id": "5Z1DsHtCr0Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: The Neural ODE (NODE) Wrapper\n",
        "# -----------------------------------------------------------------\n",
        "# This module wraps our ChemNet and calls the 'odeint' solver.\n",
        "\n",
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, derivative_net):\n",
        "        super(NeuralODE, self).__init__()\n",
        "        self.net = derivative_net\n",
        "\n",
        "    def forward(self, y0, t_points):\n",
        "        # y0 shape: (batch_size, n_features)\n",
        "        # t_points shape: (n_timesteps,)\n",
        "\n",
        "        # 'odeint' solves the ODE defined by self.net\n",
        "        # 'method='dopri5'' is a standard Runge-Kutta solver,\n",
        "        # as suggested by the Bansude paper (they used 'dopri5' for training)\n",
        "        # 'adjoint' method is used for efficient backpropagation\n",
        "        pred_y = odeint(self.net, y0, t_points, method='dopri5', rtol=1e-5, atol=1e-6) # eased the tolerances for faster inferences\n",
        "\n",
        "        # Output shape from odeint is (n_timesteps, batch_size, n_features)\n",
        "        return pred_y"
      ],
      "metadata": {
        "id": "ve89gDQHr2aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: The PyTorch Dataset and DataLoader\n",
        "\n",
        "# We create a simple Dataset to feed trajectories to our model.\n",
        "class ODESolutionDataset(Dataset):\n",
        "    def __init__(self, trajectories, device):\n",
        "        # Data shape: (num_sims, n_timesteps, n_features)\n",
        "        # We convert to float32 (standard for NN) and move to device\n",
        "        self.data = torch.tensor(trajectories, dtype=torch.float32).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns one full trajectory\n",
        "        # Shape: (n_timesteps, n_features)\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "# --- Prepare data for training ---\n",
        "\n",
        "# Convert time points to a tensor on the correct device\n",
        "t_points = torch.tensor(t_points, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = ODESolutionDataset(all_trajectories, device)\n",
        "# The DataLoader will batch trajectories\n",
        "# A 'batch' will have shape: (BATCH_SIZE, n_timesteps, n_features)\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "vQszDkNZr4nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: The Training Loop\n",
        "\n",
        "# 1. Initialize models\n",
        "derivative_net = ChemNet(N_FEATURES, HIDDEN_DIMS).to(device)\n",
        "model = NeuralODE(derivative_net).to(device)\n",
        "\n",
        "# --- LOAD SAVED MODEL IF IT EXISTS ---\n",
        "try:\n",
        "    derivative_net.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
        "    print(f\"Successfully loaded existing model from: {MODEL_SAVE_PATH}\")\n",
        "    print(\"Continuing training from this checkpoint.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"No existing model found at {MODEL_SAVE_PATH}.\")\n",
        "    print(\"Starting a new training run.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Starting a new training run.\")\n",
        "# ----------------------------------------\n",
        "\n",
        "\n",
        "# 2. Loss Function and Optimizer\n",
        "loss_function = nn.L1Loss()  # L1Loss is MAE\n",
        "\n",
        "# --- FIX 1: Define Optimizer BEFORE Scheduler ---\n",
        "# Bansude et al. used Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# This will reduce the LR by half if the loss doesn't improve for 50 epochs\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    'min',\n",
        "    factor=0.5,\n",
        "    patience=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkhfdwrmr6o0",
        "outputId": "3d1cf75d-38bc-4672-cee2-e79285f7af7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded existing model from: /content/drive/MyDrive/SciML_Project/chem_node_net_3lay_128dim.pth\n",
            "Continuing training from this checkpoint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--- Starting Training on {device} ---\")\n",
        "\n",
        "# --- Start training ---\n",
        "start_time = time.time()\n",
        "for epoch in range(N_EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    for batch_y_true in data_loader:\n",
        "        # batch_y_true shape: (BATCH_SIZE, n_timesteps, n_features)\n",
        "\n",
        "        # Get the initial condition (the state at t=0)\n",
        "        # y0 shape: (BATCH_SIZE, n_features)\n",
        "        batch_y0 = batch_y_true[:, 0, :]\n",
        "\n",
        "        # --- Forward Pass ---\n",
        "        # Run the Neural ODE\n",
        "        # pred_y shape: (n_timesteps, BATCH_SIZE, n_features)\n",
        "        pred_y = model(batch_y0, t_points)\n",
        "\n",
        "        # --- Calculate Loss ---\n",
        "        # We must re-order pred_y to match batch_y_true\n",
        "        # [timesteps, batch, features] -> [batch, timesteps, features]\n",
        "        pred_y_for_loss = pred_y.permute(1, 0, 2)\n",
        "\n",
        "        loss = loss_function(pred_y_for_loss, batch_y_true)\n",
        "\n",
        "        # --- Backward Pass ---\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(data_loader)\n",
        "\n",
        "    # --- FIX 2: Call the scheduler.step() at the end of each epoch ---\n",
        "    scheduler.step(avg_loss)\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # Updated print statement to show the learning rate\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} | Avg. Loss: {avg_loss:.6f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Training Complete ---\")\n",
        "print(f\"Total time: {(end_time - start_time):.2f} seconds\")\n",
        "\n",
        "# Step 5: Save the Trained Model\n",
        "\n",
        "# We only need to save the derivative network (ChemNet),\n",
        "# as it *is* the learned dynamics function.\n",
        "torch.save(derivative_net.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Trained model saved to: {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9I6mh5Ar9WR",
        "outputId": "2e8b6606-5f82-446f-9f5c-2f375a3c9425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Training on cuda ---\n",
            "Epoch 10/500 | Avg. Loss: 0.022785 | LR: 1.00e-03\n",
            "Epoch 20/500 | Avg. Loss: 0.023400 | LR: 1.00e-03\n",
            "Epoch 30/500 | Avg. Loss: 0.024415 | LR: 1.00e-03\n",
            "Epoch 40/500 | Avg. Loss: 0.023607 | LR: 1.00e-03\n",
            "Epoch 50/500 | Avg. Loss: 0.022379 | LR: 1.00e-03\n",
            "Epoch 60/500 | Avg. Loss: 0.024616 | LR: 1.00e-03\n",
            "Epoch 70/500 | Avg. Loss: 0.022815 | LR: 1.00e-03\n",
            "Epoch 80/500 | Avg. Loss: 0.022554 | LR: 1.00e-03\n",
            "Epoch 90/500 | Avg. Loss: 0.023405 | LR: 1.00e-03\n",
            "Epoch 100/500 | Avg. Loss: 0.023639 | LR: 1.00e-03\n",
            "Epoch 110/500 | Avg. Loss: 0.023114 | LR: 1.00e-03\n",
            "Epoch 120/500 | Avg. Loss: 0.023458 | LR: 1.00e-03\n",
            "Epoch 130/500 | Avg. Loss: 0.022754 | LR: 1.00e-03\n",
            "Epoch 140/500 | Avg. Loss: 0.022948 | LR: 1.00e-03\n",
            "Epoch 150/500 | Avg. Loss: 0.022559 | LR: 1.00e-03\n",
            "Epoch 160/500 | Avg. Loss: 0.024138 | LR: 1.00e-03\n",
            "Epoch 170/500 | Avg. Loss: 0.025850 | LR: 1.00e-03\n",
            "Epoch 180/500 | Avg. Loss: 0.023996 | LR: 1.00e-03\n",
            "Epoch 190/500 | Avg. Loss: 0.020490 | LR: 5.00e-04\n",
            "Epoch 200/500 | Avg. Loss: 0.021845 | LR: 5.00e-04\n",
            "Epoch 210/500 | Avg. Loss: 0.022627 | LR: 5.00e-04\n",
            "Epoch 220/500 | Avg. Loss: 0.022777 | LR: 5.00e-04\n",
            "Epoch 230/500 | Avg. Loss: 0.022318 | LR: 5.00e-04\n",
            "Epoch 240/500 | Avg. Loss: 0.021081 | LR: 5.00e-04\n",
            "Epoch 250/500 | Avg. Loss: 0.022099 | LR: 5.00e-04\n",
            "Epoch 260/500 | Avg. Loss: 0.021512 | LR: 5.00e-04\n",
            "Epoch 270/500 | Avg. Loss: 0.020431 | LR: 5.00e-04\n",
            "Epoch 280/500 | Avg. Loss: 0.021791 | LR: 5.00e-04\n",
            "Epoch 290/500 | Avg. Loss: 0.022419 | LR: 5.00e-04\n",
            "Epoch 300/500 | Avg. Loss: 0.021553 | LR: 5.00e-04\n",
            "Epoch 310/500 | Avg. Loss: 0.022302 | LR: 5.00e-04\n",
            "Epoch 320/500 | Avg. Loss: 0.021449 | LR: 2.50e-04\n",
            "Epoch 330/500 | Avg. Loss: 0.021067 | LR: 2.50e-04\n",
            "Epoch 340/500 | Avg. Loss: 0.020414 | LR: 2.50e-04\n",
            "Epoch 350/500 | Avg. Loss: 0.020079 | LR: 2.50e-04\n",
            "Epoch 360/500 | Avg. Loss: 0.020502 | LR: 2.50e-04\n",
            "Epoch 370/500 | Avg. Loss: 0.020857 | LR: 2.50e-04\n",
            "Epoch 380/500 | Avg. Loss: 0.020300 | LR: 2.50e-04\n",
            "Epoch 390/500 | Avg. Loss: 0.020184 | LR: 2.50e-04\n",
            "Epoch 400/500 | Avg. Loss: 0.021289 | LR: 2.50e-04\n",
            "Epoch 410/500 | Avg. Loss: 0.019632 | LR: 2.50e-04\n",
            "Epoch 420/500 | Avg. Loss: 0.020227 | LR: 2.50e-04\n",
            "Epoch 430/500 | Avg. Loss: 0.019733 | LR: 2.50e-04\n",
            "Epoch 440/500 | Avg. Loss: 0.020066 | LR: 2.50e-04\n",
            "Epoch 450/500 | Avg. Loss: 0.020852 | LR: 2.50e-04\n",
            "Epoch 460/500 | Avg. Loss: 0.021668 | LR: 1.25e-04\n",
            "Epoch 470/500 | Avg. Loss: 0.019503 | LR: 1.25e-04\n",
            "Epoch 480/500 | Avg. Loss: 0.019916 | LR: 1.25e-04\n",
            "Epoch 490/500 | Avg. Loss: 0.019559 | LR: 1.25e-04\n",
            "Epoch 500/500 | Avg. Loss: 0.019917 | LR: 1.25e-04\n",
            "--- Training Complete ---\n",
            "Total time: 3910.91 seconds\n",
            "Trained model saved to: /content/drive/MyDrive/SciML_Project/chem_node_net_3lay_128dim.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQ0ayvxwsYNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}