{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2776565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. Install torchdiffeq\n",
    "# (Run this in your terminal)\n",
    "!pip install torchdiffeq\n",
    "\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02d2aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Data loaded. Found 32 features (species + temp).\n"
     ]
    }
   ],
   "source": [
    "# Set the Configuration\n",
    "\n",
    "# File paths\n",
    "NPZ_FILE = 'training_data.npz'\n",
    "PARAMS_FILE = 'normalization_params.json'\n",
    "MODEL_SAVE_PATH = 'chem_node_net.pth'\n",
    "\n",
    "# Training hyperparameters\n",
    "N_EPOCHS = 2000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "HIDDEN_DIMS = 256  \n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Load data to get model dimensions\n",
    "\n",
    "try:\n",
    "    with np.load(NPZ_FILE) as data:\n",
    "        all_trajectories = data['trajectories']\n",
    "        t_points = data['times']\n",
    "    \n",
    "    # Get the number of features (e.g., 22 for your ammonia mech)\n",
    "    # Shape is (num_sims, num_timesteps, num_features)\n",
    "    N_FEATURES = all_trajectories.shape[2]\n",
    "    \n",
    "    print(f\"Data loaded. Found {N_FEATURES} features (species + temp).\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find '{NPZ_FILE}'.\")\n",
    "    print(\"Please make sure you have run Phase 2.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51c8c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Step 1: The Derivative Network \n",
    "\n",
    "class ChemNet(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features):\n",
    "        super(ChemNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_features, hidden_features),\n",
    "            nn.ELU(),  # Exponential Linear Unit [cite: 866]\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_features, input_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # The 'odeint' solver requires the network's forward\n",
    "        # method to accept (time, state) as arguments.\n",
    "        # We don't use 't' here, but it's a required argument.\n",
    "        return self.net(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4832244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: The Neural ODE (NODE) Wrapper\n",
    "# -----------------------------------------------------------------\n",
    "# This module wraps our ChemNet and calls the 'odeint' solver.\n",
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, derivative_net):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.net = derivative_net\n",
    "\n",
    "    def forward(self, y0, t_points):\n",
    "        # y0 shape: (batch_size, n_features)\n",
    "        # t_points shape: (n_timesteps,)\n",
    "        \n",
    "        # 'odeint' solves the ODE defined by self.net\n",
    "        # 'method='dopri5'' is a standard Runge-Kutta solver, \n",
    "        # as suggested by the Bansude paper (they used 'dopri5' for training)\n",
    "        # 'adjoint' method is used for efficient backpropagation \n",
    "        pred_y = odeint(self.net, y0, t_points, method='dopri5')\n",
    "        \n",
    "        # Output shape from odeint is (n_timesteps, batch_size, n_features)\n",
    "        return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b61b3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: The PyTorch Dataset and DataLoader\n",
    "\n",
    "# We create a simple Dataset to feed trajectories to our model.\n",
    "class ODESolutionDataset(Dataset):\n",
    "    def __init__(self, trajectories, device):\n",
    "        # Data shape: (num_sims, n_timesteps, n_features)\n",
    "        # We convert to float32 (standard for NN) and move to device\n",
    "        self.data = torch.tensor(trajectories, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns one full trajectory\n",
    "        # Shape: (n_timesteps, n_features)\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# --- Prepare data for training ---\n",
    "\n",
    "# Convert time points to a tensor on the correct device\n",
    "t_points = torch.tensor(t_points, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = ODESolutionDataset(all_trajectories, device)\n",
    "# The DataLoader will batch trajectories\n",
    "# A 'batch' will have shape: (BATCH_SIZE, n_timesteps, n_features)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d84a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: The Training Loop\n",
    "\n",
    "# 1. Initialize models\n",
    "derivative_net = ChemNet(N_FEATURES, HIDDEN_DIMS).to(device)\n",
    "model = NeuralODE(derivative_net).to(device)\n",
    "\n",
    "# 2. Loss Function and Optimizer\n",
    "loss_function = nn.L1Loss()  # L1Loss is MAE\n",
    "\n",
    "# --- FIX 1: Define Optimizer BEFORE Scheduler ---\n",
    "# Bansude et al. used Adam \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# This will reduce the LR by half if the loss doesn't improve for 50 epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    'min', \n",
    "    factor=0.5, \n",
    "    patience=50, \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training on cpu ---\n",
      "Epoch 10/2000 | Avg. Loss: 0.055133 | LR: 1.00e-03\n",
      "Epoch 20/2000 | Avg. Loss: 0.053256 | LR: 1.00e-03\n",
      "Epoch 30/2000 | Avg. Loss: 0.050271 | LR: 1.00e-03\n",
      "Epoch 40/2000 | Avg. Loss: 0.052117 | LR: 1.00e-03\n",
      "Epoch 50/2000 | Avg. Loss: 0.050759 | LR: 1.00e-03\n",
      "Epoch 60/2000 | Avg. Loss: 0.045410 | LR: 1.00e-03\n",
      "Epoch 70/2000 | Avg. Loss: 0.043345 | LR: 1.00e-03\n",
      "Epoch 80/2000 | Avg. Loss: 0.042743 | LR: 1.00e-03\n",
      "Epoch 90/2000 | Avg. Loss: 0.044099 | LR: 1.00e-03\n",
      "Epoch 100/2000 | Avg. Loss: 0.043567 | LR: 1.00e-03\n",
      "Epoch 110/2000 | Avg. Loss: 0.043214 | LR: 1.00e-03\n",
      "Epoch 120/2000 | Avg. Loss: 0.038296 | LR: 1.00e-03\n",
      "Epoch 130/2000 | Avg. Loss: 0.037354 | LR: 1.00e-03\n",
      "Epoch 140/2000 | Avg. Loss: 0.036262 | LR: 1.00e-03\n",
      "Epoch 150/2000 | Avg. Loss: 0.035840 | LR: 1.00e-03\n",
      "Epoch 160/2000 | Avg. Loss: 0.032348 | LR: 1.00e-03\n",
      "Epoch 170/2000 | Avg. Loss: 0.032558 | LR: 1.00e-03\n",
      "Epoch 180/2000 | Avg. Loss: 0.028601 | LR: 1.00e-03\n",
      "Epoch 190/2000 | Avg. Loss: 0.029126 | LR: 1.00e-03\n",
      "Epoch 200/2000 | Avg. Loss: 0.026892 | LR: 1.00e-03\n",
      "Epoch 210/2000 | Avg. Loss: 0.028518 | LR: 1.00e-03\n",
      "Epoch 220/2000 | Avg. Loss: 0.030759 | LR: 1.00e-03\n",
      "Epoch 230/2000 | Avg. Loss: 0.028556 | LR: 1.00e-03\n",
      "Epoch 240/2000 | Avg. Loss: 0.026054 | LR: 1.00e-03\n",
      "Epoch 250/2000 | Avg. Loss: 0.025481 | LR: 1.00e-03\n",
      "Epoch 260/2000 | Avg. Loss: 0.032104 | LR: 1.00e-03\n",
      "Epoch 270/2000 | Avg. Loss: 0.030741 | LR: 1.00e-03\n",
      "Epoch 280/2000 | Avg. Loss: 0.026804 | LR: 1.00e-03\n",
      "Epoch 290/2000 | Avg. Loss: 0.026158 | LR: 1.00e-03\n",
      "Epoch 300/2000 | Avg. Loss: 0.023710 | LR: 1.00e-03\n",
      "Epoch 310/2000 | Avg. Loss: 0.026917 | LR: 1.00e-03\n",
      "Epoch 320/2000 | Avg. Loss: 0.026501 | LR: 1.00e-03\n",
      "Epoch 330/2000 | Avg. Loss: 0.025784 | LR: 1.00e-03\n",
      "Epoch 340/2000 | Avg. Loss: 0.044073 | LR: 5.00e-04\n",
      "Epoch 350/2000 | Avg. Loss: 0.035092 | LR: 5.00e-04\n",
      "Epoch 360/2000 | Avg. Loss: 0.031368 | LR: 5.00e-04\n",
      "Epoch 370/2000 | Avg. Loss: 0.023784 | LR: 5.00e-04\n",
      "Epoch 380/2000 | Avg. Loss: 0.023879 | LR: 5.00e-04\n",
      "Epoch 390/2000 | Avg. Loss: 0.024336 | LR: 5.00e-04\n",
      "Epoch 400/2000 | Avg. Loss: 0.024437 | LR: 5.00e-04\n",
      "Epoch 410/2000 | Avg. Loss: 0.028267 | LR: 5.00e-04\n",
      "Epoch 420/2000 | Avg. Loss: 0.026249 | LR: 5.00e-04\n",
      "Epoch 430/2000 | Avg. Loss: 0.024743 | LR: 5.00e-04\n",
      "Epoch 440/2000 | Avg. Loss: 0.030538 | LR: 5.00e-04\n",
      "Epoch 450/2000 | Avg. Loss: 0.021650 | LR: 5.00e-04\n",
      "Epoch 460/2000 | Avg. Loss: 0.022054 | LR: 5.00e-04\n",
      "Epoch 470/2000 | Avg. Loss: 0.023742 | LR: 5.00e-04\n",
      "Epoch 480/2000 | Avg. Loss: 0.020894 | LR: 5.00e-04\n",
      "Epoch 490/2000 | Avg. Loss: 0.020359 | LR: 5.00e-04\n",
      "Epoch 500/2000 | Avg. Loss: 0.021040 | LR: 5.00e-04\n",
      "Epoch 510/2000 | Avg. Loss: 0.019783 | LR: 5.00e-04\n",
      "Epoch 520/2000 | Avg. Loss: 0.021531 | LR: 5.00e-04\n",
      "Epoch 530/2000 | Avg. Loss: 0.025927 | LR: 5.00e-04\n",
      "Epoch 540/2000 | Avg. Loss: 0.024889 | LR: 5.00e-04\n",
      "Epoch 550/2000 | Avg. Loss: 0.021604 | LR: 5.00e-04\n",
      "Epoch 560/2000 | Avg. Loss: 0.022649 | LR: 5.00e-04\n",
      "Epoch 570/2000 | Avg. Loss: 0.018624 | LR: 5.00e-04\n",
      "Epoch 580/2000 | Avg. Loss: 0.020357 | LR: 5.00e-04\n",
      "Epoch 590/2000 | Avg. Loss: 0.020707 | LR: 5.00e-04\n",
      "Epoch 600/2000 | Avg. Loss: 0.033501 | LR: 5.00e-04\n",
      "Epoch 610/2000 | Avg. Loss: 0.021184 | LR: 5.00e-04\n",
      "Epoch 620/2000 | Avg. Loss: 0.018543 | LR: 5.00e-04\n",
      "Epoch 630/2000 | Avg. Loss: 0.021117 | LR: 5.00e-04\n",
      "Epoch 640/2000 | Avg. Loss: 0.019282 | LR: 2.50e-04\n",
      "Epoch 650/2000 | Avg. Loss: 0.018840 | LR: 2.50e-04\n",
      "Epoch 660/2000 | Avg. Loss: 0.019804 | LR: 2.50e-04\n",
      "Epoch 670/2000 | Avg. Loss: 0.019150 | LR: 2.50e-04\n",
      "Epoch 680/2000 | Avg. Loss: 0.017114 | LR: 2.50e-04\n",
      "Epoch 690/2000 | Avg. Loss: 0.026771 | LR: 2.50e-04\n",
      "Epoch 700/2000 | Avg. Loss: 0.019175 | LR: 2.50e-04\n",
      "Epoch 710/2000 | Avg. Loss: 0.016614 | LR: 2.50e-04\n",
      "Epoch 720/2000 | Avg. Loss: 0.016031 | LR: 2.50e-04\n",
      "Epoch 730/2000 | Avg. Loss: 0.016830 | LR: 2.50e-04\n",
      "Epoch 740/2000 | Avg. Loss: 0.017808 | LR: 2.50e-04\n",
      "Epoch 750/2000 | Avg. Loss: 0.015849 | LR: 2.50e-04\n",
      "Epoch 760/2000 | Avg. Loss: 0.019500 | LR: 2.50e-04\n",
      "Epoch 770/2000 | Avg. Loss: 0.017000 | LR: 2.50e-04\n",
      "Epoch 780/2000 | Avg. Loss: 0.015462 | LR: 2.50e-04\n",
      "Epoch 790/2000 | Avg. Loss: 0.016666 | LR: 2.50e-04\n",
      "Epoch 800/2000 | Avg. Loss: 0.017026 | LR: 2.50e-04\n",
      "Epoch 810/2000 | Avg. Loss: 0.019300 | LR: 2.50e-04\n",
      "Epoch 820/2000 | Avg. Loss: 0.015868 | LR: 2.50e-04\n",
      "Epoch 830/2000 | Avg. Loss: 0.015353 | LR: 1.25e-04\n",
      "Epoch 840/2000 | Avg. Loss: 0.015731 | LR: 1.25e-04\n",
      "Epoch 850/2000 | Avg. Loss: 0.014828 | LR: 1.25e-04\n",
      "Epoch 860/2000 | Avg. Loss: 0.014523 | LR: 1.25e-04\n",
      "Epoch 870/2000 | Avg. Loss: 0.015076 | LR: 1.25e-04\n",
      "Epoch 880/2000 | Avg. Loss: 0.015695 | LR: 6.25e-05\n",
      "Epoch 890/2000 | Avg. Loss: 0.015874 | LR: 6.25e-05\n",
      "Epoch 900/2000 | Avg. Loss: 0.015092 | LR: 6.25e-05\n",
      "Epoch 910/2000 | Avg. Loss: 0.015220 | LR: 6.25e-05\n",
      "Epoch 920/2000 | Avg. Loss: 0.015447 | LR: 6.25e-05\n",
      "Epoch 930/2000 | Avg. Loss: 0.015439 | LR: 6.25e-05\n",
      "Epoch 940/2000 | Avg. Loss: 0.014964 | LR: 3.13e-05\n",
      "Epoch 950/2000 | Avg. Loss: 0.016274 | LR: 3.13e-05\n",
      "Epoch 960/2000 | Avg. Loss: 0.015001 | LR: 3.13e-05\n",
      "Epoch 970/2000 | Avg. Loss: 0.014094 | LR: 3.13e-05\n",
      "Epoch 980/2000 | Avg. Loss: 0.015338 | LR: 3.13e-05\n",
      "Epoch 990/2000 | Avg. Loss: 0.014801 | LR: 3.13e-05\n",
      "Epoch 1000/2000 | Avg. Loss: 0.016904 | LR: 3.13e-05\n",
      "Epoch 1010/2000 | Avg. Loss: 0.015102 | LR: 3.13e-05\n",
      "Epoch 1020/2000 | Avg. Loss: 0.016068 | LR: 3.13e-05\n",
      "Epoch 1030/2000 | Avg. Loss: 0.014237 | LR: 1.56e-05\n",
      "Epoch 1040/2000 | Avg. Loss: 0.015461 | LR: 1.56e-05\n",
      "Epoch 1050/2000 | Avg. Loss: 0.018143 | LR: 1.56e-05\n",
      "Epoch 1060/2000 | Avg. Loss: 0.015962 | LR: 1.56e-05\n",
      "Epoch 1070/2000 | Avg. Loss: 0.014908 | LR: 1.56e-05\n",
      "Epoch 1080/2000 | Avg. Loss: 0.014857 | LR: 1.56e-05\n",
      "Epoch 1090/2000 | Avg. Loss: 0.014165 | LR: 1.56e-05\n",
      "Epoch 1100/2000 | Avg. Loss: 0.016748 | LR: 1.56e-05\n",
      "Epoch 1110/2000 | Avg. Loss: 0.016137 | LR: 1.56e-05\n",
      "Epoch 1120/2000 | Avg. Loss: 0.015738 | LR: 7.81e-06\n",
      "Epoch 1130/2000 | Avg. Loss: 0.015168 | LR: 7.81e-06\n",
      "Epoch 1140/2000 | Avg. Loss: 0.015145 | LR: 7.81e-06\n",
      "Epoch 1150/2000 | Avg. Loss: 0.016666 | LR: 7.81e-06\n",
      "Epoch 1160/2000 | Avg. Loss: 0.015728 | LR: 7.81e-06\n",
      "Epoch 1170/2000 | Avg. Loss: 0.015826 | LR: 7.81e-06\n",
      "Epoch 1180/2000 | Avg. Loss: 0.016820 | LR: 7.81e-06\n",
      "Epoch 1190/2000 | Avg. Loss: 0.014498 | LR: 7.81e-06\n",
      "Epoch 1200/2000 | Avg. Loss: 0.015636 | LR: 7.81e-06\n",
      "Epoch 1210/2000 | Avg. Loss: 0.016739 | LR: 3.91e-06\n",
      "Epoch 1220/2000 | Avg. Loss: 0.015146 | LR: 3.91e-06\n",
      "Epoch 1230/2000 | Avg. Loss: 0.015635 | LR: 3.91e-06\n",
      "Epoch 1240/2000 | Avg. Loss: 0.014623 | LR: 3.91e-06\n",
      "Epoch 1250/2000 | Avg. Loss: 0.013854 | LR: 3.91e-06\n",
      "Epoch 1260/2000 | Avg. Loss: 0.014439 | LR: 1.95e-06\n",
      "Epoch 1270/2000 | Avg. Loss: 0.015518 | LR: 1.95e-06\n",
      "Epoch 1280/2000 | Avg. Loss: 0.014326 | LR: 1.95e-06\n",
      "Epoch 1290/2000 | Avg. Loss: 0.016675 | LR: 1.95e-06\n",
      "Epoch 1300/2000 | Avg. Loss: 0.014991 | LR: 1.95e-06\n",
      "Epoch 1310/2000 | Avg. Loss: 0.014217 | LR: 9.77e-07\n",
      "Epoch 1320/2000 | Avg. Loss: 0.016379 | LR: 9.77e-07\n",
      "Epoch 1330/2000 | Avg. Loss: 0.015324 | LR: 9.77e-07\n",
      "Epoch 1340/2000 | Avg. Loss: 0.015477 | LR: 9.77e-07\n",
      "Epoch 1350/2000 | Avg. Loss: 0.015883 | LR: 9.77e-07\n",
      "Epoch 1360/2000 | Avg. Loss: 0.015864 | LR: 4.88e-07\n",
      "Epoch 1370/2000 | Avg. Loss: 0.015453 | LR: 4.88e-07\n",
      "Epoch 1380/2000 | Avg. Loss: 0.016031 | LR: 4.88e-07\n",
      "Epoch 1390/2000 | Avg. Loss: 0.016837 | LR: 4.88e-07\n",
      "Epoch 1400/2000 | Avg. Loss: 0.015324 | LR: 4.88e-07\n",
      "Epoch 1410/2000 | Avg. Loss: 0.015429 | LR: 2.44e-07\n",
      "Epoch 1420/2000 | Avg. Loss: 0.015239 | LR: 2.44e-07\n",
      "Epoch 1430/2000 | Avg. Loss: 0.014359 | LR: 2.44e-07\n",
      "Epoch 1440/2000 | Avg. Loss: 0.016220 | LR: 2.44e-07\n",
      "Epoch 1450/2000 | Avg. Loss: 0.013915 | LR: 2.44e-07\n",
      "Epoch 1460/2000 | Avg. Loss: 0.016121 | LR: 1.22e-07\n",
      "Epoch 1470/2000 | Avg. Loss: 0.016045 | LR: 1.22e-07\n",
      "Epoch 1480/2000 | Avg. Loss: 0.015641 | LR: 1.22e-07\n",
      "Epoch 1490/2000 | Avg. Loss: 0.015296 | LR: 1.22e-07\n",
      "Epoch 1500/2000 | Avg. Loss: 0.015002 | LR: 1.22e-07\n",
      "Epoch 1510/2000 | Avg. Loss: 0.015182 | LR: 6.10e-08\n",
      "Epoch 1520/2000 | Avg. Loss: 0.015800 | LR: 6.10e-08\n",
      "Epoch 1530/2000 | Avg. Loss: 0.013977 | LR: 6.10e-08\n",
      "Epoch 1540/2000 | Avg. Loss: 0.015507 | LR: 6.10e-08\n",
      "Epoch 1550/2000 | Avg. Loss: 0.014156 | LR: 6.10e-08\n",
      "Epoch 1560/2000 | Avg. Loss: 0.015454 | LR: 3.05e-08\n",
      "Epoch 1570/2000 | Avg. Loss: 0.014445 | LR: 3.05e-08\n",
      "Epoch 1580/2000 | Avg. Loss: 0.015697 | LR: 3.05e-08\n",
      "Epoch 1590/2000 | Avg. Loss: 0.014994 | LR: 3.05e-08\n",
      "Epoch 1600/2000 | Avg. Loss: 0.014547 | LR: 3.05e-08\n",
      "Epoch 1610/2000 | Avg. Loss: 0.015603 | LR: 3.05e-08\n",
      "Epoch 1620/2000 | Avg. Loss: 0.015154 | LR: 1.53e-08\n",
      "Epoch 1630/2000 | Avg. Loss: 0.015479 | LR: 1.53e-08\n",
      "Epoch 1640/2000 | Avg. Loss: 0.015674 | LR: 1.53e-08\n",
      "Epoch 1650/2000 | Avg. Loss: 0.017673 | LR: 1.53e-08\n",
      "Epoch 1660/2000 | Avg. Loss: 0.016747 | LR: 1.53e-08\n",
      "Epoch 1670/2000 | Avg. Loss: 0.016353 | LR: 1.53e-08\n",
      "Epoch 1680/2000 | Avg. Loss: 0.017128 | LR: 1.53e-08\n",
      "Epoch 1690/2000 | Avg. Loss: 0.014898 | LR: 1.53e-08\n",
      "Epoch 1700/2000 | Avg. Loss: 0.015463 | LR: 1.53e-08\n",
      "Epoch 1710/2000 | Avg. Loss: 0.015573 | LR: 1.53e-08\n",
      "Epoch 1720/2000 | Avg. Loss: 0.015411 | LR: 1.53e-08\n",
      "Epoch 1730/2000 | Avg. Loss: 0.014049 | LR: 1.53e-08\n",
      "Epoch 1740/2000 | Avg. Loss: 0.015740 | LR: 1.53e-08\n",
      "Epoch 1750/2000 | Avg. Loss: 0.014448 | LR: 1.53e-08\n",
      "Epoch 1760/2000 | Avg. Loss: 0.016952 | LR: 1.53e-08\n",
      "Epoch 1770/2000 | Avg. Loss: 0.014308 | LR: 1.53e-08\n",
      "Epoch 1780/2000 | Avg. Loss: 0.015736 | LR: 1.53e-08\n",
      "Epoch 1790/2000 | Avg. Loss: 0.015153 | LR: 1.53e-08\n",
      "Epoch 1800/2000 | Avg. Loss: 0.014279 | LR: 1.53e-08\n",
      "Epoch 1810/2000 | Avg. Loss: 0.015968 | LR: 1.53e-08\n",
      "Epoch 1820/2000 | Avg. Loss: 0.016346 | LR: 1.53e-08\n",
      "Epoch 1830/2000 | Avg. Loss: 0.014440 | LR: 1.53e-08\n",
      "Epoch 1840/2000 | Avg. Loss: 0.016326 | LR: 1.53e-08\n",
      "Epoch 1850/2000 | Avg. Loss: 0.014238 | LR: 1.53e-08\n",
      "Epoch 1860/2000 | Avg. Loss: 0.016704 | LR: 1.53e-08\n",
      "Epoch 1870/2000 | Avg. Loss: 0.014132 | LR: 1.53e-08\n",
      "Epoch 1880/2000 | Avg. Loss: 0.014302 | LR: 1.53e-08\n",
      "Epoch 1890/2000 | Avg. Loss: 0.018032 | LR: 1.53e-08\n",
      "Epoch 1900/2000 | Avg. Loss: 0.017278 | LR: 1.53e-08\n",
      "Epoch 1910/2000 | Avg. Loss: 0.017174 | LR: 1.53e-08\n",
      "Epoch 1920/2000 | Avg. Loss: 0.013763 | LR: 1.53e-08\n",
      "Epoch 1930/2000 | Avg. Loss: 0.017501 | LR: 1.53e-08\n",
      "Epoch 1940/2000 | Avg. Loss: 0.015917 | LR: 1.53e-08\n",
      "Epoch 1950/2000 | Avg. Loss: 0.016683 | LR: 1.53e-08\n",
      "Epoch 1960/2000 | Avg. Loss: 0.017716 | LR: 1.53e-08\n",
      "Epoch 1970/2000 | Avg. Loss: 0.016375 | LR: 1.53e-08\n",
      "Epoch 1980/2000 | Avg. Loss: 0.016697 | LR: 1.53e-08\n",
      "Epoch 1990/2000 | Avg. Loss: 0.014504 | LR: 1.53e-08\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Starting Training on {device} ---\")\n",
    "\n",
    "# --- Start training ---\n",
    "start_time = time.time()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for batch_y_true in data_loader:\n",
    "        # batch_y_true shape: (BATCH_SIZE, n_timesteps, n_features)\n",
    "        \n",
    "        # Get the initial condition (the state at t=0)\n",
    "        # y0 shape: (BATCH_SIZE, n_features)\n",
    "        batch_y0 = batch_y_true[:, 0, :]\n",
    "        \n",
    "        # --- Forward Pass ---\n",
    "        # Run the Neural ODE\n",
    "        # pred_y shape: (n_timesteps, BATCH_SIZE, n_features)\n",
    "        pred_y = model(batch_y0, t_points)\n",
    "        \n",
    "        # --- Calculate Loss ---\n",
    "        # We must re-order pred_y to match batch_y_true\n",
    "        # [timesteps, batch, features] -> [batch, timesteps, features]\n",
    "        pred_y_for_loss = pred_y.permute(1, 0, 2)\n",
    "        \n",
    "        loss = loss_function(pred_y_for_loss, batch_y_true)\n",
    "\n",
    "        # --- Backward Pass ---\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    \n",
    "    # --- FIX 2: Call the scheduler.step() at the end of each epoch ---\n",
    "    scheduler.step(avg_loss)\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # Updated print statement to show the learning rate\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} | Avg. Loss: {avg_loss:.6f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"--- Training Complete ---\")\n",
    "print(f\"Total time: {(end_time - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c9b4601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to: chem_node_net.pth\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save the Trained Model\n",
    "\n",
    "# We only need to save the derivative network (ChemNet),\n",
    "# as it *is* the learned dynamics function.\n",
    "torch.save(derivative_net.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Trained model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55782c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
